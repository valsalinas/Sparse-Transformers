# Sparse-Transformers
Survey/Problem Statement

Transformer models are commonly used in machine learning for tasks such as natural language processing, computer vision, and other areas that involve sequential data. While they are highly effective, their attention mechanism can become very slow and consume a large amount of memory when working with long sequences. This can make it challenging to process lengthy documents, extended conversations, or large datasets efficiently.
Sparse Transformer models aim to address this problem by reducing the number of attention connections. Instead of allowing every part of the input to interact with every other part, sparse attention focuses only on the most important connections. This approach lowers both computational and memory costs while still maintaining strong performance.
This survey will focus on Sparse Attention in Transformers for Long Sequences. It will examine early research that introduced sparse attention concepts as well as more recent studies that improve or apply these methods in practice. The survey will categorize approaches based on how sparsity is implemented and how efficiently the methods operate. It will also discuss how well different approaches balance speed and accuracy.
The main question driving this survey is, how can Transformers process very long sequences efficiently without losing important information or accuracy? While sparse attention improves efficiency, it can also make it harder for the model to capture the full context. This survey aims to summarize current research, compare different methods, and outline challenges as well as potential areas for future work.
